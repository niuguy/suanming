{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/59/d88fe8c58ffb66aca21d03c0e290cd68327cc133591130c674985e98a482/tensorflow-1.14.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "Collecting mock>=2.0.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/69/46/ebd21ce467ab87f2cf825413273936f9b1ee0e6cd4e2f2ee62e0516c771f/grpcio-1.25.0-cp27-cp27mu-manylinux1_x86_64.whl (2.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.5MB 409kB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/37/e6a7af1c92c5b68fb427f853b06164b56ea92126bcfd87784334ec5e4d42/tensorboard-1.14.0-py2-none-any.whl\n",
      "Collecting enum34>=1.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/68/eed71ec8e8383a43987e0a58492b0aa043dc29971cac59b8b1d1e7951123/google_pasta-0.1.8-py2-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 13.2MB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting wheel (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/04/4e36c33f8eb5c5b6c622a1f4859352a6acca7ab387257d4b3c191d23ec1d/gast-0.3.2.tar.gz\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/49/ffa7ab9c52ec56b535cffec3bc844254c073888e6d4aeee464671ac97480/protobuf-3.10.0-cp27-cp27mu-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.2MB/s \n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 13.2MB/s \n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 541kB/s \n",
      "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting futures>=2.2.0; python_version < \"3.2\" (from grpcio>=1.8.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 4.8MB/s \n",
      "\u001b[?25hCollecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/de/554b6310ac87c5b921bc45634b07b11394fe63bc4cb5176f5240addf18ab/setuptools-41.6.0-py2.py3-none-any.whl (582kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 2.6MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: gast, absl-py\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/59/38/c6/234dc39b4f6951a0768fbc02d5b7207137a5b1d9094f0d54bf\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "Successfully built gast absl-py\n",
      "Installing collected packages: numpy, six, h5py, keras-applications, funcsigs, mock, futures, enum34, grpcio, termcolor, setuptools, protobuf, wheel, absl-py, werkzeug, markdown, tensorboard, google-pasta, tensorflow-estimator, astor, backports.weakref, gast, keras-preprocessing, wrapt, tensorflow\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.0 backports.weakref-1.0.post1 enum34-1.1.6 funcsigs-1.0.2 futures-3.3.0 gast-0.3.2 google-pasta-0.1.8 grpcio-1.25.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.16.5 protobuf-3.10.0 setuptools-41.6.0 six-1.13.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import data_util\n",
    "from model_components import task_specific_attention, bidirectional_rnn\n",
    "import pandas as pd\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANClassifierModel():\n",
    "  \"\"\" Implementation of document classification model described in\n",
    "    `Hierarchical Attention Networks for Document Classification (Yang et al., 2016)`\n",
    "    (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               embedding_size,\n",
    "               classes,\n",
    "               word_cell,\n",
    "               sentence_cell,\n",
    "               word_output_size,\n",
    "               sentence_output_size,\n",
    "               max_grad_norm,\n",
    "               dropout_keep_proba,\n",
    "               is_training=None,\n",
    "               learning_rate=1e-4,\n",
    "               device='/cpu:0',\n",
    "               scope=None):\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_size = embedding_size\n",
    "    self.classes = classes\n",
    "    self.word_cell = word_cell\n",
    "    self.word_output_size = word_output_size\n",
    "    self.sentence_cell = sentence_cell\n",
    "    self.sentence_output_size = sentence_output_size\n",
    "    self.max_grad_norm = max_grad_norm\n",
    "    self.dropout_keep_proba = dropout_keep_proba\n",
    "\n",
    "    with tf.variable_scope(scope or 'tcm') as scope:\n",
    "      self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "      if is_training is not None:\n",
    "        self.is_training = is_training\n",
    "      else:\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "      self.sample_weights = tf.placeholder(shape=(None,), dtype=tf.float32, name='sample_weights')\n",
    "\n",
    "      # [document x sentence x word]\n",
    "      self.inputs = tf.placeholder(shape=(None, None, None), dtype=tf.int32, name='inputs')\n",
    "\n",
    "      # [document x sentence]\n",
    "      self.word_lengths = tf.placeholder(shape=(None, None), dtype=tf.int32, name='word_lengths')\n",
    "\n",
    "      # [document]\n",
    "      self.sentence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='sentence_lengths')\n",
    "\n",
    "      # [document]\n",
    "      self.labels = tf.placeholder(shape=(None,), dtype=tf.int32, name='labels')\n",
    "\n",
    "      (self.document_size,\n",
    "        self.sentence_size,\n",
    "        self.word_size) = tf.unstack(tf.shape(self.inputs))\n",
    "\n",
    "      self._init_embedding(scope)\n",
    "\n",
    "      # embeddings cannot be placed on GPU\n",
    "      with tf.device(device):\n",
    "        self._init_body(scope)\n",
    "\n",
    "    with tf.variable_scope('train'):\n",
    "      self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n",
    "\n",
    "#       self.loss = tf.reduce_mean(tf.multiply(self.cross_entropy, self.sample_weights))\n",
    "        \n",
    "      self.loss = tf.reduce_mean(self.cross_entropy)\n",
    "\n",
    "      tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "      self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.labels, 1), tf.float32))\n",
    "      tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "      tvars = tf.trainable_variables()\n",
    "\n",
    "      grads, global_norm = tf.clip_by_global_norm(\n",
    "        tf.gradients(self.loss, tvars),\n",
    "        self.max_grad_norm)\n",
    "      tf.summary.scalar('global_grad_norm', global_norm)\n",
    "\n",
    "      opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "      self.train_op = opt.apply_gradients(\n",
    "        zip(grads, tvars), name='train_op',\n",
    "        global_step=self.global_step)\n",
    "\n",
    "      self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "  def _init_embedding(self, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "      with tf.variable_scope(\"embedding\") as scope:\n",
    "        self.embedding_matrix = tf.get_variable(\n",
    "          name=\"embedding_matrix\",\n",
    "          shape=[self.vocab_size, self.embedding_size],\n",
    "          initializer=layers.xavier_initializer(),\n",
    "          dtype=tf.float32)\n",
    "        self.inputs_embedded = tf.nn.embedding_lookup(\n",
    "          self.embedding_matrix, self.inputs)\n",
    "\n",
    "  def _init_body(self, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "\n",
    "      word_level_inputs = tf.reshape(self.inputs_embedded, [\n",
    "        self.document_size * self.sentence_size,\n",
    "        self.word_size,\n",
    "        self.embedding_size\n",
    "      ])\n",
    "      word_level_lengths = tf.reshape(\n",
    "        self.word_lengths, [self.document_size * self.sentence_size])\n",
    "\n",
    "      with tf.variable_scope('word') as scope:\n",
    "        word_encoder_output, _ = bidirectional_rnn(\n",
    "          self.word_cell, self.word_cell,\n",
    "          word_level_inputs, word_level_lengths,\n",
    "          scope=scope)\n",
    "\n",
    "        with tf.variable_scope('attention') as scope:\n",
    "          word_level_output = task_specific_attention(\n",
    "            word_encoder_output,\n",
    "            self.word_output_size,\n",
    "            scope=scope)\n",
    "\n",
    "        with tf.variable_scope('dropout'):\n",
    "          word_level_output = layers.dropout(\n",
    "            word_level_output, keep_prob=self.dropout_keep_proba,\n",
    "            is_training=self.is_training,\n",
    "          )\n",
    "\n",
    "      # sentence_level\n",
    "\n",
    "      sentence_inputs = tf.reshape(\n",
    "        word_level_output, [self.document_size, self.sentence_size, self.word_output_size])\n",
    "\n",
    "      with tf.variable_scope('sentence') as scope:\n",
    "        sentence_encoder_output, _ = bidirectional_rnn(\n",
    "          self.sentence_cell, self.sentence_cell, sentence_inputs, self.sentence_lengths, scope=scope)\n",
    "\n",
    "        with tf.variable_scope('attention') as scope:\n",
    "          sentence_level_output = task_specific_attention(\n",
    "            sentence_encoder_output, self.sentence_output_size, scope=scope)\n",
    "\n",
    "        with tf.variable_scope('dropout'):\n",
    "          sentence_level_output = layers.dropout(\n",
    "            sentence_level_output, keep_prob=self.dropout_keep_proba,\n",
    "            is_training=self.is_training,\n",
    "          )\n",
    "\n",
    "      with tf.variable_scope('classifier'):\n",
    "        self.logits = layers.fully_connected(\n",
    "          sentence_level_output, self.classes, activation_fn=None)\n",
    "\n",
    "        self.prediction = tf.argmax(self.logits, axis=-1)\n",
    "\n",
    "  def get_feed_data(self, x, y=None, class_weights=None, is_training=True):\n",
    "    x_m, doc_sizes, sent_sizes = data_util.batch(x)\n",
    "    fd = {\n",
    "      self.inputs: x_m,\n",
    "      self.sentence_lengths: doc_sizes,\n",
    "      self.word_lengths: sent_sizes,\n",
    "    }\n",
    "    if y is not None:\n",
    "      fd[self.labels] = y\n",
    "      if class_weights is not None:\n",
    "        fd[self.sample_weights] = [class_weights[yy] for yy in y]\n",
    "      else:\n",
    "        fd[self.sample_weights] = np.ones(shape=[len(x_m)], dtype=np.float32)\n",
    "    fd[self.is_training] = is_training\n",
    "    return fd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\x11\\n\\ntrain/loss\\x15\\x08\\xd4/?\\n\\x15\\n\\x0etrain/accuracy\\x15\\x00\\x00\\x80?\\n\\x1d\\n\\x16train/global_grad_norm\\x15iN\\x14>'\n",
      "0.6868291\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "except ImportError:\n",
    "        LSTMCell = tf.nn.rnn_cell.LSTMCell\n",
    "        LSTMStateTuple = tf.nn.rnn_cell.LSTMStateTuple\n",
    "        GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    model = HANClassifierModel(\n",
    "      vocab_size=10,\n",
    "      embedding_size=5,\n",
    "      classes=2,\n",
    "      word_cell=GRUCell(10),\n",
    "      sentence_cell=GRUCell(10),\n",
    "      word_output_size=10,\n",
    "      sentence_output_size=10,\n",
    "      max_grad_norm=5.0,\n",
    "      dropout_keep_proba=0.5,\n",
    "    )\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    fd = {\n",
    "      model.is_training: False,\n",
    "      model.inputs: [[\n",
    "        [5, 4, 1, 0],\n",
    "        [3, 3, 6, 7],\n",
    "        [6, 7, 0, 0]\n",
    "      ],\n",
    "        [\n",
    "        [2, 2, 1, 0],\n",
    "        [3, 3, 6, 7],\n",
    "        [0, 0, 0, 0]\n",
    "      ]],\n",
    "      model.word_lengths: [\n",
    "        [3, 4, 2],\n",
    "        [3, 4, 0],\n",
    "      ],\n",
    "      model.sentence_lengths: [3, 2],\n",
    "      model.labels: [0, 1],\n",
    "    }\n",
    "\n",
    "#     print(session.run(model.logits, fd))\n",
    "    step, summaries, loss, accuracy, _ = session.run([\n",
    "          model.global_step,\n",
    "          model.summary_op,\n",
    "          model.loss,\n",
    "          model.accuracy,\n",
    "          model.train_op,\n",
    "      ], fd)\n",
    "    print(summaries)\n",
    "    print(loss)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='dataset/dtoc.pkl'):\n",
    "    pd_dtoc = pickle.load(open(filename, 'rb'))\n",
    "    return pd_dtoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = load_data(filename='/home/jupyter/rich/dtoc_proc.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sp_no</th>\n",
       "      <th>ep_no</th>\n",
       "      <th>start_date</th>\n",
       "      <th>diag1</th>\n",
       "      <th>diag2</th>\n",
       "      <th>diag3</th>\n",
       "      <th>diag4</th>\n",
       "      <th>diag5</th>\n",
       "      <th>diag6</th>\n",
       "      <th>...</th>\n",
       "      <th>proc12</th>\n",
       "      <th>spell_time</th>\n",
       "      <th>dest_code</th>\n",
       "      <th>adm_code</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>loe</th>\n",
       "      <th>is_oversea</th>\n",
       "      <th>los</th>\n",
       "      <th>is_dtoc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>M1355572</td>\n",
       "      <td>62295900</td>\n",
       "      <td>1</td>\n",
       "      <td>2005-02-16 00:00:00</td>\n",
       "      <td>Z349</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-02-16 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4232</td>\n",
       "      <td>0</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>M581130</td>\n",
       "      <td>62784072</td>\n",
       "      <td>1</td>\n",
       "      <td>2008-04-11 00:00:00</td>\n",
       "      <td>Z349</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008-04-11 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2303</td>\n",
       "      <td>0</td>\n",
       "      <td>2303.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>M1588346</td>\n",
       "      <td>63195747</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-05 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-10-05 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>1241</td>\n",
       "      <td>0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>M1587381</td>\n",
       "      <td>63354756</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-10-03 00:00:00</td>\n",
       "      <td>O701</td>\n",
       "      <td>Z370</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-10-03 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>M1564074</td>\n",
       "      <td>63357984</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-10-10 00:00:00</td>\n",
       "      <td>O701</td>\n",
       "      <td>Z370</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-10-10 00:00:00</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1774</td>\n",
       "      <td>0</td>\n",
       "      <td>1774.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195236</td>\n",
       "      <td>5087844</td>\n",
       "      <td>4313925</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-31 00:49:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-31 00:49:00</td>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>30011</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195237</td>\n",
       "      <td>1253471</td>\n",
       "      <td>4313971</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-31 00:59:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-31 00:59:00</td>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>30011</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195238</td>\n",
       "      <td>5281325</td>\n",
       "      <td>4313949</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-31 01:41:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-31 01:41:00</td>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>30011</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195239</td>\n",
       "      <td>3034556</td>\n",
       "      <td>4314014</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-31 01:42:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-31 01:42:00</td>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30011</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195240</td>\n",
       "      <td>552915</td>\n",
       "      <td>4314028</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-10-31 01:51:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-10-31 01:51:00</td>\n",
       "      <td>98</td>\n",
       "      <td>21</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>30011</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1195241 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id     sp_no  ep_no          start_date diag1 diag2 diag3  \\\n",
       "0        M1355572  62295900      1 2005-02-16 00:00:00  Z349  None  None   \n",
       "1         M581130  62784072      1 2008-04-11 00:00:00  Z349  None  None   \n",
       "2        M1588346  63195747      1 2010-10-05 00:00:00  None  None  None   \n",
       "3        M1587381  63354756      1 2011-10-03 00:00:00  O701  Z370  None   \n",
       "4        M1564074  63357984      1 2011-10-10 00:00:00  O701  Z370  None   \n",
       "...           ...       ...    ...                 ...   ...   ...   ...   \n",
       "1195236   5087844   4313925      1 2018-10-31 00:49:00  None  None  None   \n",
       "1195237   1253471   4313971      1 2018-10-31 00:59:00  None  None  None   \n",
       "1195238   5281325   4313949      1 2018-10-31 01:41:00  None  None  None   \n",
       "1195239   3034556   4314014      1 2018-10-31 01:42:00  None  None  None   \n",
       "1195240    552915   4314028      1 2018-10-31 01:51:00  None  None  None   \n",
       "\n",
       "        diag4 diag5 diag6  ... proc12          spell_time dest_code adm_code  \\\n",
       "0        None  None  None  ...   None 2005-02-16 00:00:00        19       31   \n",
       "1        None  None  None  ...   None 2008-04-11 00:00:00        19       31   \n",
       "2        None  None  None  ...   None 2010-10-05 00:00:00        19       31   \n",
       "3        None  None  None  ...   None 2011-10-03 00:00:00        19       31   \n",
       "4        None  None  None  ...   None 2011-10-10 00:00:00        19       31   \n",
       "...       ...   ...   ...  ...    ...                 ...       ...      ...   \n",
       "1195236  None  None  None  ...   None 2018-10-31 00:49:00        98       21   \n",
       "1195237  None  None  None  ...   None 2018-10-31 00:59:00        98       21   \n",
       "1195238  None  None  None  ...   None 2018-10-31 01:41:00        98       21   \n",
       "1195239  None  None  None  ...   None 2018-10-31 01:42:00        98       21   \n",
       "1195240  None  None  None  ...   None 2018-10-31 01:51:00        98       21   \n",
       "\n",
       "        age gender    loe is_oversea     los is_dtoc  \n",
       "0        18      2   4232          0  4232.0     0.0  \n",
       "1        28      2   2303          0  2303.0     0.0  \n",
       "2        36      2   1241          0  1241.0     0.0  \n",
       "3        19      2    864          0   864.0     0.0  \n",
       "4        31      2   1774          0  1774.0     0.0  \n",
       "...      ..    ...    ...        ...     ...     ...  \n",
       "1195236  88      1  30011          1     NaN     0.0  \n",
       "1195237  32      2  30011          0     NaN     0.0  \n",
       "1195238  13      1  30011          0     NaN     0.0  \n",
       "1195239   0      2  30011          0     NaN     0.0  \n",
       "1195240  67      1  30011          1     NaN     0.0  \n",
       "\n",
       "[1195241 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
